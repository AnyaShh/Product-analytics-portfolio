# A/B тест: влияние изменения алгоритма на CTR

## Контекст
Есть две версии алгоритма рекомендаций. Хотим понять, улучшает ли новая версия взаимодействие с лентой.

## Цель
Оценить влияние нового алгоритма на **CTR** и принять решение о раскатке.

## Данные
Логи показов и кликов пользователей в ленте за период эксперимента.  
Единица анализа — **пользователь** (user-level).

## Метрики
**Primary:** CTR (clicks / impressions) на пользователя  
**Guardrails (пример):** DAU/кол-во показов на пользователя, time spent / depth (если доступны)

## Дизайн эксперимента
- Контроль: текущий алгоритм  
- Тест: новый алгоритм  
- Рандомизация: по пользователю  
- Уровень значимости: 0.05

## Проверка корректности сплита (A/A)
Перед A/B проверила, что система рандомизации работает корректно:
- провела A/A тест
- распределение p-value близко к равномерному
- доля p-value < 0.05 ≈ 0.05 (ожидаемо)

Вывод: **сплит корректный**, можно доверять A/B.

## Анализ A/B
Для оценки эффекта на CTR использовала:
- статистические сравнения между группами
- **bootstrap** (в т.ч. пуассоновский бутстрап)
- **бакетизацию** для стабилизации метрики

Считала эффект в абсолютных и относительных величинах, оценивала доверительные интервалы.

## Результат и решение
CTR в тестовой группе **ухудшился** → **раскатывать на всех пользователей не стоит**.

## Next steps
- проверить эффект по сегментам (new vs returning, платформа, гео)
- проверить корректность логирования показов/кликов (особенно если CTR “ломается”)
- рассмотреть гибридный запуск: ограниченная раскатка или fallback на старый алгоритм

## Стек
Python (pandas, scipy), статистика/бутстрап, A/B, Jupyter
